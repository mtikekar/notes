{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoLayers in PyTorch\n",
    "\n",
    "AutoLayers automatically adjust their shape, device and dtype based on the first input provided to them. For example, AutoConv2d does not need to be told its input channel count; it infers that from its first input.\n",
    "\n",
    "It works by saving the arguments used to initialize the actual layer and deferring initialization until the first forward pass.\n",
    "In the first forward pass, the layer is initialized based on the input tensor's shape, dtype and device.\n",
    "Further, I use Python's `__class__` trick to convert the AutoLayer into PyTorch's standard layer.\n",
    "\n",
    "You can freely mix standard PyTorch layers and AutoLayers in a module. Just remember to pass one example input to initialize the AutoLayers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoLayer(nn.Module):\n",
    "    def __init__(self, *args, **kw):\n",
    "        self._autoargs = deepcopy(args)\n",
    "        self._autokw = deepcopy(kw)\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, *args, **kw):\n",
    "        # convert self from AutoLayer to actual layer\n",
    "        self.__class__ = self._autocls\n",
    "\n",
    "        # initialize layer now\n",
    "        self.__init__(args[0].shape[1], *self._autoargs, **self._autokw)\n",
    "        self.to(args[0])\n",
    "\n",
    "        # run forward pass as if nothing happened\n",
    "        return self.forward(*args, **kw)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        alist = ['_'] + [repr(a) for a in self._autoargs]\n",
    "        alist += [k + '=' + repr(v) for k,v in self._autokw.items()]\n",
    "        return ', '.join(alist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoConv2d(AutoLayer):\n",
    "    _autocls = nn.Conv2d\n",
    "\n",
    "class AutoLinear(AutoLayer):\n",
    "    _autocls = nn.Linear\n",
    "\n",
    "class AutoBatchNorm2d(AutoLayer):\n",
    "    _autocls = nn.BatchNorm2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all it takes to define AutoLayers!\n",
    "\n",
    "Now, we define an example CNN. For demonstration, we use a skip-concat connection. We will also change the activation tensors' device and dtype a few times through the network and see how the layers are automatically adjusted to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = AutoConv2d(32, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = AutoBatchNorm2d()\n",
    "        self.conv2 = AutoConv2d(64, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = AutoBatchNorm2d()\n",
    "        self.fc = AutoLinear(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.bn1(y)\n",
    "        y = F.relu(y)\n",
    "\n",
    "        # move to GPU\n",
    "        y = y.to('cuda')\n",
    "        y1 = F.max_pool2d(y, 2)\n",
    "\n",
    "        y = self.conv2(y1)\n",
    "        y = self.bn2(y)\n",
    "        y = F.relu(y)\n",
    "        y = torch.cat([y1, y], dim=1)\n",
    "\n",
    "        y = F.max_pool2d(y, 2)\n",
    "        y = y.view(y.shape[0], -1)\n",
    "\n",
    "        # do fully connected layer in higher precision for fun\n",
    "        y = y.to(torch.float64)\n",
    "\n",
    "        return self.fc(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model as usual. The only difference is that the model starts out \"empty\". After one input is provided, it gets properly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net1()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(torch.randn(1, 3, 32, 32))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the layers have changed from `AutoConv2d` to `Conv2d`. Also note how the device and dtype of the three layers have been adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1.weight.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv2.weight.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc.weight.type()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch (conda)",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
